%%%%%%%%%%%%%
%% CONTEXT %%
%%%%%%%%%%%%%

%This chapter explains the current state of your topic, in practice and theory. This is the state of the world which you intend to improve, and the state of knowledge on top of which you build your advances and from which you learn knowledge to apply and constraints on your work. So, you will report and analyse what is known about a certain topic, as reported in reference literature and published scientific literature; if you are developing a product, you will need to report about comparable or competing products over which you intend to improve or from which you will obtain ideas; you may need to describe legal or societal situation within which your work takes place; etc.  
  
%It is important to demonstrate scholarship, i.e. the ability to read about a subject area in a range of sources, assimilate the material and then discuss it intelligently.  
  
%You should demonstrate that you understand what you have read by providing some analysis or commentary in view of the goals of your project: it is not enough simply to provide summaries of what you have read. References should be cited following the Harvard Referencing Style. You must also explain, both in this chapter and, as appropriate, in others, how the results of the studies to which you make reference inform your project work. To gain a passing grade, your report MUST demonstrate adequate engagement with academic literature and any other sources necessary for the work to be well informed.  

% The story we want to tell
% (...)

\chapter{Context}
\label{Context} 

%%%%%%%%%%%%%%%%%%%%
% RELATED WORK
%%%%%%%%%%%%%%%%%%%%

"Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp -- a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to real-world images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms' performance on transparent objects." \cite{sajjan2019cleargrasp}.

RGB-D cameras.

\cite{RakhimkulEtAl2019} Assistive robot solutions are mostly designed as robot helpers with robotic arms and aim to assist disabled and elderly people with carrying out basic activities of daily life such as reaching household objects, i.e. cups, feeding with spoon,opening a drawer/fridge doors, etc., However, commercial assistive robotic arms with joystick control require extensive and tiring hand motor skill training that limits robotâ€™s practical usage by patients with disabilities. The main objective of this work is to present the methodology for designing an intelligent human-machine interface for a commercial joystick controlled assistive robotic arm realizing shared autonomy and supervisory control modes. Preliminary results on a RGB-D based object detection and position estimation system development using publicly available YOLOv3 and CenterNet deep learning models implementation of an autonomous object grasping mode by the Kinova Jaco robotic arm are described in detail and experimentally demonstrated.

\cite{batzner2021se3equivariant} guys say that small datasets work ok.\cite{power2021simple} also found this, and \cite{vu2021machine}. These guys \cite{frankle2019lottery} discuss small networks

% Conferences

% https://www.mhnetwork.com/conference-on-collaborative-robots-advanced-vision-and-artificial-intelligence/

\section{Related Work}
\label{context:related-work} 






