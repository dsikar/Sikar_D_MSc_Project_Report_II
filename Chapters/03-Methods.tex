%%%%%%%%%%%%%
%% METHODS %%
%%%%%%%%%%%%%

%This chapter describes in detail the methods for whatever activities were necessary for your project â€“ e.g., data gathering, data analysis, requirements analysis, design, implementation, testing/evaluation, etc. Your choice of methods should be discussed and justified in view of the project objectives, and with reference to the pertinent literature. Report not only what methods you applied in generic terms, but what you actually did: sufficient information about dates and details for your reader to understand how you ran your project, rather than just how one could run any similar project. 

\chapter{Methods}
\label{Methods} 

This chapter describes the methods used to generate the prediction model.

\section{conda}

% https://docs.conda.io/projects/conda/en/latest/index.html

conda (\cite{Conda2021}) is a system that simplifies package management and deployment. It is part of the Anaconda Python distribution. conda allows environments to be created, saved and switched from one to another, such that separate environments can be configured and run. A list of conda commands used in this project is given in \ref{app:methods:conda}. While conda can act as a package installer, it is only used here as an environment manager, while pip is used a a package manager.

\section{pip}

pip (\cite{pip2021}) is a package installer for Python that can be used to install packages from PyPI (the Python Package Index), a repository for the Python programming language, used to distribute software written in Python.

\section{UR3}
% https://www.universal-robots.com/products/ur3-robot/

The original experiment used a UR3 robotic arm
% Tinker Braccio - max load 150g
% https://store.arduino.cc/tinkerkit-braccio-robot
% will probably have to go with that plus a wall mount scenario
% https://www.zivid.com/zivid-one-plus
% SDKs
% https://www.zivid.com/downloads

%%%%%%%%%%%%%%%
% SETUP
%%%%%%%%%%%%%%%
% Hardware
% 1. Zivid One+ camera in fixed position
% 2. Robotic Arm TBA
% 3. Transparent, translucent and opaque object

% Software
% 1. Tensorflow/Keras
% 2. ROS
% 3. Unity

% Models
% 1. Trained network for object detection
% 2. Trained network for path planning / object grasping
% 3. Trained network for command speech recognition - NTH

% All image and path planning data to be generated by Unity
% All audio data to be generated by GAN-like architectures, from a few audio samples

\section{Zivid One+}

\section{Niryo One}
% Train end-to-end to pick-and-place ? :D

% The story we are trying to tell
% 1. We'll retrieve an image RBD + RGBD from camera
% 2. The image x2 goes through a pipeline where the output is a prediction of the 3D space and objects present
% 3. A voice issues a command like move object from A to B
% 4. The 3D space and command are fed to another pipeline, which outputs a path
% the robotic arm needs to follow to move an object from location A to location B
\section{ROS}

\section{ROS x Arduino}

% https://maker.pro/arduino/tutorial/how-to-use-arduino-with-robot-operating-system-ros

% http://wiki.ros.org/rosserial_arduino/Tutorials

\section{Unity}
% Robotics' simulation in Unity
% https://resources.unity.com/unitenow/onlinesessions/simulating-robots-with-ros-and-unity

% Niryo One Robotic Arm in Unity
% https://blogs.unity3d.com/2020/11/19/robotics-simulation-in-unity-is-as-easy-as-1-2-3/

% ROS#
% https://github.com/siemens/ros-sharp

\section{Network Architectures}

In this section we discuss the network architectures used by ClearGrasp and Depth2Depth.



\section{Depth Cameras}

\begin{verbatim}
Start here: https://www.intelrealsense.com/beginners-guide-to-depth/
and take it from there.
Maybe also have a look here:
https://ulir.ul.ie/bitstream/handle/10344/7592/Sivcev_2018_Smart.pdf?sequence=5
Point GrayBumblebee 2 stereo camera
Point Gray BlackFly monocular camera
\end{verbatim}


\section{Prediction Workflow}

\begin{verbatim}
1. Input
2. Networks

eval_depth_completion.py, line 169:

We get the "corrected" output depth likesuch:

output_depth, filtered_output_depth = depthcomplete.depth_completion(
(...)

depth_completion is a method of the DepthToDepthCompletion class,
defined in:

depth_completion_api.py, line 732:
    def depth_completion(self,
    (...)
    which returns numpy arrays:
    
        return self.output_depth, self.filtered_output_depth

Note, these are to compute error metrics, the actual images
are stored during the inference process.

To obtain the two arrays:    
    # perform object segmentation
    def _modify_depth_delete_masks(

 571       self.mask_predicted = self.inferenceMasks.runOnNumpyImage
 
 
 _complete_depth
 
 # SURFACE NORMALS
 (...) self.inferenceNormals.runOnNumpyImage(input_image)
 
 Then, perform boundary detection:
 # OUTLINES
        self.occlusion_weight, self.occlusion_weight_rgb, self.outlines_rgb = self.inferenceOutlines.runOnNumpyImage(
            input_image)
            
            
 
 self.InferenceMasks
 
 3 models
 inference_models.InferenceNormals
 inference_models.InferenceOutlines(
 inference_models.InferenceMasks(
 
In inference_models.py:
 
self.model = deeplab_masks.DeepLab(num_classes=3, backbone='resnet', sync_bn=True,
                            freeze_bn=False)
                            
we get deeplab_masks from

from .modeling import deeplab, deeplab_masks

Network architectures used by each task (normal estimation, boundary detection and 
transparent object segmentation).

InferenceOutlines -> deeplab_resnet

InferenceNormals -> deeplab_resnet

InferenceMasks -> drn

In all three cases, image is presented as an one-dimensional array.
(...)
# Create Transforms
        self.transform = iaa.Sequential([
(...)


\end{verbatim}

\section{OpenEXR}
\begin{verbatim}
https://www.openexr.com/index.html - need a citation

https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=openexr+image+file+format&btnG=&oq=openexr
\end{verbatim}

OpenEXR is not playing well with windows. TODO Try on Ubuntu


