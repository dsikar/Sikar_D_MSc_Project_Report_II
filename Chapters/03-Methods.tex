%%%%%%%%%%%%%
%% METHODS %%
%%%%%%%%%%%%%

%This chapter describes in detail the methods for whatever activities were necessary for your project â€“ e.g., data gathering, data analysis, requirements analysis, design, implementation, testing/evaluation, etc. Your choice of methods should be discussed and justified in view of the project objectives, and with reference to the pertinent literature. Report not only what methods you applied in generic terms, but what you actually did: sufficient information about dates and details for your reader to understand how you ran your project, rather than just how one could run any similar project. 

\chapter{Methods}
\label{Methods} 

This chapter describes the methods used to generate the prediction model.

\section{conda}

% https://docs.conda.io/projects/conda/en/latest/index.html

conda (\cite{Conda2021}) is a system that simplifies package management and deployment. It is part of the Anaconda Python distribution. conda allows environments to be created, saved and switched from one to another, such that separate environments can be configured and run. A list of conda commands used in this project is given in \ref{app:methods:conda}. While conda can act as a package installer, it is only used here as an environment manager, while pip is used a a package manager.

\section{pip}

pip (\cite{pip2021}) is a package installer for Python that can be used to install packages from PyPI (the Python Package Index), a repository for the Python programming language, used to distribute software written in Python.

\section{UR3}
% https://www.universal-robots.com/products/ur3-robot/

The original experiment used a UR3 robotic arm
% Tinker Braccio - max load 150g
% https://store.arduino.cc/tinkerkit-braccio-robot
% will probably have to go with that plus a wall mount scenario
% https://www.zivid.com/zivid-one-plus
% SDKs
% https://www.zivid.com/downloads

%%%%%%%%%%%%%%%
% SETUP
%%%%%%%%%%%%%%%
% Hardware
% 1. Zivid One+ camera in fixed position
% 2. Robotic Arm TBA
% 3. Transparent, translucent and opaque object

% Software
% 1. Tensorflow/Keras
% 2. ROS
% 3. Unity

% Models
% 1. Trained network for object detection
% 2. Trained network for path planning / object grasping
% 3. Trained network for command speech recognition - NTH

% All image and path planning data to be generated by Unity
% All audio data to be generated by GAN-like architectures, from a few audio samples

\section{Zivid One+}

\section{Niryo One}
% Train end-to-end to pick-and-place ? :D

% The story we are trying to tell
% 1. We'll retrieve an image RBD + RGBD from camera
% 2. The image x2 goes through a pipeline where the output is a prediction of the 3D space and objects present
% 3. A voice issues a command like move object from A to B
% 4. The 3D space and command are fed to another pipeline, which outputs a path
% the robotic arm needs to follow to move an object from location A to location B
\section{ROS}

\section{ROS x Arduino}

% https://maker.pro/arduino/tutorial/how-to-use-arduino-with-robot-operating-system-ros

% http://wiki.ros.org/rosserial_arduino/Tutorials

\section{Unity}
% Robotics' simulation in Unity
% https://resources.unity.com/unitenow/onlinesessions/simulating-robots-with-ros-and-unity

% Niryo One Robotic Arm in Unity
% https://blogs.unity3d.com/2020/11/19/robotics-simulation-in-unity-is-as-easy-as-1-2-3/

% ROS#
% https://github.com/siemens/ros-sharp

