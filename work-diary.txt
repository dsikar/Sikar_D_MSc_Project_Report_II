2021.01.04

Created Template

2021.01.05

Uploaded project proposal

2021.01.23

Read "ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation".

2021.01.24

Replicating experiment

2021.01.25

Replicated experiment ok on Ubuntu 18.04. TODO try 20.04 tomorrow.

Note, using a new conda environment might have something to do with this relatively
painless install, compared to first.

2021.01.25

Replicating experiment on Ubuntu 20.04.
Failing - missing libraries.

2021.01.28

Determining a number of Transparent Object Identification CNNs.

This is shaping up to be a "Comparison of Transparent Object Identification using RGB-D cameras and CNNs"

Looking into training a robot end-to-end to move objects.

2021.02.07

Another angle, Unity has has a Robotic Hub on GitHub:

https://github.com/Unity-Technologies/Unity-Robotics-Hub

which integrates with ROS. One possibility is, using the simulator,
complete with robotic arm and sensor cameras, acquire the image, be in RGB
or RGBD, present it to processing pipeline and feedback to simulation
for object manipulation, and evaluate the pipelines that way. The expectation
being better pipelines will generate better path planning.
Note that the transparent objects are placed within unity.

2021.02.13

Downloaded Zivid for Ubuntu packages as per instructions on:
https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/59080712/Zivid+Software+Installation

Section "Ubuntu 18.04"

2021.02.20

Going through Zivid Academy:
https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/99844339/Zivid+Academy

Keywords: Structured light.

Error when running ZividStudio:

"An error occurred: Error occurred during initialization: An OpenCL error occurred: Failed to get platforms

Make sure the OpenCL driver is installed and working ('clinfo' needs to show at least one platform).

For more help, visit the Zivid Knowledge Base at https://help.zivid.com.

[CL_PLATFORM_NOT_FOUND_KHR]"

Recommended fix https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/428165/OpenCL+Driver+Check

For Ubuntu:

"Intel (Ubuntu)

Separately check OpenCL and OpenGL driver versions."

OpenCL Driver Check, ran:

sudo apt install -y clinfo
 
/usr/bin/clinfo

Output as expected:

(base) simbox@simbox-wifi-server:~/Documents/zivid$ /usr/bin/clinfo
Number of platforms                               0

OpenGL Driver Check, ran:

sudo apt install mesa-utils
 
glxinfo | grep "OpenGL version"

Output:

(base) simbox@simbox-wifi-server:~/Downloads$ glxinfo | grep "OpenGL version"
OpenGL version string: 4.3 (Compatibility Profile) Mesa 20.0.8

Version 4.3, higher than 3.0 so presumed ok.

When running ZividStudio

cd /usr/bin
(base) simbox@simbox-wifi-server:/usr/bin$ sudo ./ZividStudio 
Segmentation fault

Restarted, ran ZividStudio from Ubuntu start, same error:

"An error occurred: Error occurred during initialization: An OpenCL error occurred: Failed to get platforms

Make sure the OpenCL driver is installed and working ('clinfo' needs to show at least one platform).

For more help, visit the Zivid Knowledge Base at https://help.zivid.com.

[CL_PLATFORM_NOT_FOUND_KHR]"

Tried from command line:

(base) simbox@simbox-wifi-server:/usr/bin$ sudo ./ZividStudio
Segmentation fault

in /var/log/syslog:

"Feb 20 13:08:02 simbox-wifi-server kernel: [  283.591172] traps: ZividStudio[4195] general protection fault ip:7fa51a2cfa46 sp:7ffef69ac390 error:0 in libZividCore.so[7fa519fbe000+38d1000]
"
Another more verbose error message:

"(base) simbox@simbox-wifi-server:/usr/bin$ ./ZividStudio 
onFatalError :  "Error occurred during initialization: An OpenCL error occurred: Failed to get platforms\n\nMake sure the OpenCL driver is installed and working ('clinfo' needs to show at least one platform).\n\nFor more help, visit the Zivid Knowledge Base at https://help.zivid.com.\n\n[CL_PLATFORM_NOT_FOUND_KHR]"
showErrorMessageBoxAndThrow
Error: An error occurred: Error occurred during initialization: An OpenCL error occurred: Failed to get platforms

Make sure the OpenCL driver is installed and working ('clinfo' needs to show at least one platform).

For more help, visit the Zivid Knowledge Base at https://help.zivid.com.

[CL_PLATFORM_NOT_FOUND_KHR]"

Reread Open CL Driver check:
https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/428165/OpenCL+Driver+Check

With note "In case the number of platforms is equal to 0 you need to install the OpenCL driver."

Current CPU:

"(base) simbox@simbox-wifi-server:/usr/bin$ cat /proc/cpuinfo | grep "model name"
model name	: Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz"

Xeon(R) CPU E5-1650, which does not have integrated graphics according to:
https://www.techpowerup.com/cpu-specs/xeon-e5-1650.c984

And

https://www.intel.com/content/www/us/en/processors/xeon/xeon-autodesk-workstation-brief.html

While Zivid page states options for Ubuntu:

https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/428084/GPU+Requirements

Trying the python script:

(base) simbox@simbox-wifi-server:~/git/zivid$ pip install zivid --user
Collecting zivid
Requirement already satisfied: numpy in /home/simbox/anaconda3/lib/python3.7/site-packages (from zivid) (1.17.2)
Installing collected packages: zivid
Successfully installed zivid-2.1.0.2.2.0

Then suggested hello world to load library:

(base) simbox@simbox-wifi-server:~/git/zivid$ python hellozivid.py 
Segmentation fault (core dumped)

Options:

1. Get a workstation with Integrated CPU e.g. Intel i7 with HD630
2. Get graphics card e.g. Nvidia GeForce GTX 1060 - ordered.

2021.02.28

Nvidia GeForce GTX 1060 card install.

Updated drivers by running:

# Check
$ ubuntu-drivers devices
# sudo ubuntu-drivers autoinstall

Swapped NVIDIA gm206glm Quatro M2200 card for GeForce GTX 1060 then powered up.
To check devices:
daniel@simbox:~$ nvidia-smi
Sun Feb 28 22:13:08 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  Off  | 00000000:03:00.0  On |                  N/A |
|  0%   35C    P8     6W / 120W |    283MiB /  6077MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A       939      G   /usr/lib/xorg/Xorg                156MiB |
|    0   N/A  N/A      1194      G   /usr/bin/gnome-shell              103MiB |
|    0   N/A  N/A      1433      G   ...setup/gnome-initial-setup       19MiB |
+-----------------------------------------------------------------------------+

2021.03.14

Going over previous step from 2021.02.13

This time we have an NVIDIA GPU card installed, previous was an Intel GPU but for reasons not understood did not play with Zivid software.

Ideally we would have just swapped cards, but Ubuntu install was required to get NVIDIA 
GeForce GTX 1060 6GB card working.

https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/59080712/Zivid+Software+Installation

1. Installing NVIDIA GPU drivers

https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/58294334/NVIDIA+GPU

$ sudo add-apt-repository ppa:graphics-drivers/ppa

$ sudo apt install nvidia-driver-430

# checking number of platforms
$ sudo apt install -y clinfo
$ /usr/bin/clinfo
Number of platforms                               1
(...)

Above covers first section "Before installing Zivid Software". Next we go to section
"Select your OS and Zivid camera:" then to Zivid One and Ubuntu 18.04

$ wget -i https://www.zivid.com/hubfs/softwarefiles/releases/2.2.0+f0867d62-1/u16/ZividInstall1604.txt

$ sudo dpkg -i *.deb

Now zivid studio can be launched by searching "zivid" in Ubuntu launch. A firmware upgrade is
performed. An error message popped up about USB 3 being required to connect camera.
The Dell Precision T5810 does have one USB 3 port:
https://www.dell.com/support/kbdoc/en-uk/000125679/dell-precision-t5810-workstation-visual-guide-to-your-computer

Once connected to USB 3, Zivid Studio is able to make acquisitions and create point cloud, colour
and depth files.

Python scripting

https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/427542/Zivid+Python

pip install zivid failed with message:
ModuleNotFoundError: No module named 'skbuild'

We then tried installing from source:
$ git clone git@github.com:zivid/zivid-python.git
$ cd zivid-python/
$ pip install .

Got the same error message, then fixed with 
$ pip install scikit-build

Rerunning the install threw error:

!! Missing module 'cmake' !!
Please install 'cmake' manually or use PIP>=19

This could have been an oversight  from

https://zivid.atlassian.net/wiki/spaces/ZividKB/pages/427556/Setting+up+Python

So maybe we forgot to install a C/C++ compiler:

$ sudo apt install clang --fix-missing
$ clang --version
(...)

Same error thrown. So we tried to install cmake:

$ sudo snap install cmake --classic

Same error message. So we upgraded pip installed
$ python -m pip install -U pip

then

$ pip install zivid
And that worked. So perhaps should have updated pip first (probably another oversight of mine when reading Zivid docs)

2021.03.15

Python scripting

Cloned Zivid python repo:

$ git clone git@github.com:zivid/zivid-python.git
$ cd zivid-python/samples
# capture a point cloud
$ python sample_capture.py
# capture rgb image
$ python sample_capture_2d.py

Photos with a glass cup using zivid studio show the camera is unable to capture a point
adequately as described by Sajjan et al. 2019.

2021.03.20

Installing cleargrasp (again), side by side with zivid tools.

1. From https://github.com/dsikar/cleargrasp, ran: 
1.1
$ . dependencies.sh

This failed to install libhdf5-10 libhdf5-cpp-11
which are Ubuntu 16 packages:
https://packages.ubuntu.com/search?keywords=libhdf&searchon=names&suite=xenial&section=all
The corresponding 18.04 packages being  
https://packages.ubuntu.com/search?keywords=libhdf&searchon=names&suite=bionic&section=all
so the edited line in dependencies.sh reads:
sudo apt-get install -y libhdf5-100 libhdf5-serial-dev libhdf5-dev libhdf5-cpp-100

We also needed the opengl package - see following notes.
$ sudo apt-get install libglu1-mesa-dev freeglut3-dev mesa-common-dev

1.2
$ pip install -r requirements_v2.txt

1.3 Downloaded data
$ wget http://clkgum.com/shreeyak/cleargrasp-checkpoints
$ unzip cleargrasp-checkpoints

1.4 Find 

$ find /usr -iname "*hdf5.h*"
/usr/include/opencv2/hdf/hdf5.hpp
/usr/include/opencv2/flann/hdf5.h

sudo apt install libopencv-dev python-opencv

When trying to "make", we got error:

../../pkgs/RNBasics/RNGrfx.h:42:21: fatal error: GL/glu.h: No such file or directory
 #           include <GL/glu.h>

After following some advice on:
https://askubuntu.com/questions/306703/compile-opengl-program-missing-gl-gl-h
(Todo, try the "apt-file search "gl.h"" method described.

And ran:
sudo apt-get install libglu1-mesa-dev freeglut3-dev mesa-common-dev

Then make was able to compile depth2depth.

Executable test, a minor edit (using "." instead of "bash"):
cd api/depth2depth/gaps
. depth2depth.sh
Executing with bash throws and error, 

eval_depth_completion.py

2021.03.22

Some work on scholarly and semanticscholar python libraries to automate literature review.
https://github.com/dsikar/scholarly

Then:
1. Visualising point cloud using Python:
Following example in:
http://www.open3d.org/docs/0.9.0/tutorial/Basic/pointcloud.html

Locating fragment.py and referenced .ply files
>>> import open3d
>>> print(open3d.__file__)
/home/daniel/.local/lib/python3.6/site-packages/open3d/__init__.py

$ ls /home/daniel/.local/lib/python3.6/site-packages/open3d/
_build_config.py  core.py  cpu  cuda  __init__.py  j_visualizer.py  libc++abi.so.1  libc++.so.1  ml  _ml3d  __pycache__  resources  static  visualization

$ grep -nr fragment.py /home/daniel/.local/lib/python3.6/site-packages/open3d/
(no matches)

Maybe in numpy?

$ python
Python 3.6.9 (default, Jan 26 2021, 15:33:00) 
[GCC 8.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy
>>> print(numpy.__file__)
/home/daniel/.local/lib/python3.6/site-packages/numpy/__init__.py

$ grep -nr pointcloud.py  /home/daniel/.local/lib/python3.6/site-packages/numpy
(no matches)

Point cloud exploratory data analysis done in:
github.com/dsikar/zivid/PointCloudExploratoryDataAnalysis.ipynb

# Loading zivid .zdf
https://github.com/zivid/zivid-python-samples
https://github.com/zivid/zivid-python-samples/blob/master/source/applications/basic/visualization/read_zdf_vis_3d.py

Next:
1. Load .ply
2. Run ZividOne+ data through cleargrasp

2021.03.27

We have python examples here:
https://github.com/zivid/zivid-python-samples
We have .zdf (zivid capture format) examples here:
https://www.zivid.com/images

The intellisense camera used by ClearGrasp uses the .exr format:
https://www.openexr.com/about.html

So somehow, we need to figure out how to make the .zdf format play with the current codebase, which uses .exr.

Downloading a dataset of Intel's realsense RGBD images from:
http://www.cs.technion.ac.il/~twerd/HandNet/

To see what they look like.

Managed to display one network, looks like an auto-encoder. See Appendix1-Methods.tex, \section{PyTorch}.
This was the Normal Estimation (InferenceNormals class) network. 

TODO: Display Boundary Detection and Transparent Object Segmentation networks. By page 2 in
https://arxiv.org/pdf/1910.02550.pdf, there are also expected to be auto-encoders.

To get there, run the eval_depth_completion configuration in PyCharm and set breakpoints in file 
depth_completion_api.py, lines 663 (InferenceOutlines - Boundary Detection)  and 595 (InferenceMasks - Transparent Object Segmentation), then stepping into the functions.

Note, to display other networks with command:

summary(self.model, input_size=(1, 3, 144, 256))

When execution is stopped, command must be run in debugger console (where local variables are available), not in Python
Console (where local variables are not available).


2021.03.31

Got network architectures for InferenceOutlines and InferenceMasks. 

Next, write-up then present a ZividOne+ depth image and RGB image to network.

2021.04.06

Running the cleargrasp script we have:

python eval_depth_completion.py -c config/config.yaml

In config/config.yaml:

(...)
  - image: '../data/sample_dataset/real-val/d435/'
    depth: '../data/sample_dataset/real-val/d435/'
    masks: '../data/sample_dataset/real-val/d435/'
    gt_depth: '../data/sample_dataset/real-val/d435/'
(...)
eval_depth_completion.py will parse these attributes:

image, depth, masks and ground truth depth.

# Let's take a look at the .exr files:
https://stackoverflow.com/questions/55556277/read-process-and-show-the-pixels-in-exr-format-images

Todo, download .exr realsense datasets and run live_demo.py on image and depth files.
Downloaded to data/realsense/ - quite a few pairs to choose from e.g.:
042_color.png
042_depth_open.png
042_depth.png

2021.04.07 

Intel RealSense D435
https://www.intelrealsense.com/depth-camera-d435/

One related dataset (Novkovic et al. 2019, minus transparent objects):
https://clubs.github.io/

Another paper on RGB-D:
http://chalearnlap.cvc.uab.es/dataset/34/description/
4 out of 5 references are from github including a minimal implementation of PyTorch-YOLOv3:
https://github.com/eriklindernoren/PyTorch-YOLOv3

Downloaded cleargrasp's test and validation datasets:
http://clkgum.com/shreeyak/cleargrasp-dataset-test

Which contain .exr files:
https://www.openexr.com/index.html

import OpenEXR
assumed to be working on Ubuntu 18.04 as it is used in api/utils.py

2021.04.08

Todo. Check if zivid python lib can save .exr files.

****************
** 2021.04.17 **
****************

* 1. Action previous todo (2021.08.08)

Main source:
https://github.com/zivid/zivid-python

Additional findings (related to semantic versioning, which Zivid uses):

On the meaning of "major", "minor" and "bug fixes" w.r.t. semantic versioning:
https://semver.org/

On the meaning of keywords:
"MUST", "MUST NOT", "REQUIRED", "SHALL", "SHAL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED",  "MAY", and "OPTIONAL" w.r.t. software development.
https://tools.ietf.org/html/rfc2119

One key operation is given by example:

https://github.com/zivid/zivid-python#examples

where a camera capture is turned into a point cloud numpy array:
rgba = frame.point_cloud().copy_data("rgba") # Get point colors as [Height,Width,4] uint8 array

Another key operation is given in:

https://github.com/zivid/zivid-python/tree/master/samples

Specifically:

https://github.com/zivid/zivid-python/blob/master/samples/sample_capture_2d.py

Where a 2D image is obtained. With the point cloud and the 2d image, we should be able
to modify the cleargrasp code to use Zivid One+ camera data.

Additional parameters still need to be considered e.g. "hand to eye" calibration, (robot
pose) and camera intrinsics, however, the 2d image and point cloud files should be sufficient to substitute the data in:

https://github.com/Shreeyak/cleargrasp/blob/master/live_demo/live_demo.py

Lines:
    while True:
        # Get Frame. Expected format: ColorImg -> (H, W, 3) uint8, DepthImg -> (H, W) float64
        color_img, input_depth = rcamera.get_data()
        input_depth = input_depth.astype(np.float32)

From this point onwards we should be able to piggyback on the execution. Optimally,
acquire a D400 realsense camera, to have a better idea of what the data looks like,
and adjust accordingly to work with the Zivid One+

Intel D400 series specification:
https://www.mouser.co.uk/datasheet/2/612/Intel_RealSense_D400_Family_Datasheet_Jan2019-1571271.pdf

Option 1. Intel RealSense D415, supplied with tripod and cabme:
https://www.intelrealsense.com/depth-camera-d415/

Option 2. Module - custom USB adapter cable required:
https://www.intelrealsense.com/stereo-depth-modules-and-processors/

Noting that there are plenty of datasets around that should give us an idea of the output,
both depth and image.

So, to action the todo, we need to look at .exr files (the depth output of the Intel Realsene series, and determine how they compare to the ZividOne+ depth, as saved in .zdf file format.

* 2. Time remaining:

April 17, 18, 24, 25
May 1, 2, 3, 8, 9, 15, 16, 22, 23, 29, 30, 31
June 7, 8, 9, 10, 11 (part-time), 19, 20, 26, 27 (hard deadline)
NB Zivid One+ loaned camera to be returned in June

* 3. Report structure

Adding lorem ipsum text to flesh it out

****************
** 2021.04.19 **
****************

* 1. Autoencoders

As ClearGrasp and the model it is based on are based on autoencoders, We need a detailed discussion on autoencoders, high level in introduction, more detailed in methods. It would make sense to start with the base model "Deep Depth Completion of a Single RGB-D Image" from http://deepcompletion.cs.princeton.edu/, git repository https://github.com/yindaz/DeepCompletionRelease

****************
** 2021.04.20 **
****************

Continuing, we looked at the pdf:
https://arxiv.org/pdf/1803.09326.pdf

And added some snippets to Methods section (to be expanded).

****************
** 2021.04.21 **
****************

Continuing with autoencoders, there are two references for the network architecture, one is ClearGrasp:
https://github.com/Shreeyak/cleargrasp

The other is the DeepCompletion project, which ClearGrasp extends:
https://github.com/yindaz/DeepCompletionRelease

ClearGrasp defines the autoencoders (PyTorch) in:
https://github.com/Shreeyak/cleargrasp/tree/master/pytorch_networks

DeepCompletion (Lua/Torch) in:
https://github.com/yindaz/DeepCompletionRelease/blob/master/torch/model_deep.lua
https://github.com/yindaz/DeepCompletionRelease/blob/master/torch/model_deep_new.lua

The DeepCompletion paper has a discussion on network architecture (pg. 4): 
https://arxiv.org/pdf/1803.09326.pdf

****************
** 2021.04.22 **
****************

Continuing with the DeepCompletion paper, there are experimental studies in the appendix, explaining choices made for ground truth, loss function and depth representation choices pg 13.

****************
** 2021.04.24 **
****************

Installing the librerealsense drivers on Ubuntu 18.04:
https://github.com/IntelRealSense/librealsense/blob/master/doc/distribution_linux.md

When running the demo, we got an error:

$ ./build/realsense 
Listening...
realsense: /build/glfw3-KYdbSD/glfw3-3.2.1/src/window.c:774: glfwSetWindowUserPointer: Assertion `window != NULL' failed.
Aborted (core dumped)

Debugging, first step, does camera work? Demo worked after reboot.

Next step, run live_demo.py on pycharm, stop on lines:

color_img, input_depth = rcamera.get_data()
input_depth = input_depth.astype(np.float32)

Note size and data types for color_img, input_depth and add to report.
Next, visualise point cloud. Then same for Zivid One+, finally swap one for the other and run pipeline.

****************
** 2021.04.25 **
****************

Saved realsense image and depth outputs, making a note of dimensions and adding to Methods. Had some problems with Jupyter Notebook and opencv, so parking some of the planned visualisations for now and carrying on with Zivid One+.

To get the input_depth, from

https://github.com/zivid/zivid-python/blob/master/modules/zivid/point_cloud.py

    def copy_data(self, data_format):
        (...)
        z:          ndarray(Height,Width)   of float

So:
xyz = frame.point_cloud().copy_data("xyz") # Get point coordinates as [Height,Width,3] float array

We have a file ~/git/zivid/create_depth_map.py

We need to create a function that will return an image and input map, at 720 * 1280 * 3 and 720 * 1280, then call that on line 99 of live_demo.py, noting that the expected input map is a numpy array of data type float32.

****************
** 2021.04.26 **
****************

Reading the RealSense 415 datasheet:
https://www.mouser.com/pdfdocs/Intel_D400_Series_Datasheet.pdf

" In addition to depth video stream, it can provide color, and infrared video streams."

" The left and right imagers capture the scene and sends imager data to the depth imaging processor, which calculates depth values for each pixel in the image by correlating points on the left image to the right image and via shift between a point on the Left image and the Right image. The depth pixel values are processed to generate a depth frame."

Table 3-1. Format Z [16 bits] "Depth Only Mode", assumption is depth is 16 bit value.

GUI tools provided in SDK 2.0:
Intel® RealSense™ Viewer– A GUI based application for a quick evaluation of the RealSense camera.
Depth Quality Test tool for Intel® RealSense™ Camera– A GUI based application for testing the camera’s depth quality
SDK:
https://github.com/IntelRealSense/librealsense

Calibration Support:
The name of the tool is: Intel® RealSense™ Dynamic Calibrator, please find it on https://downloadcenter.intel.com/
There are two types of dynamic calibrations that are supported by the tool:
1.Targeted Dynamic Calibration (Depth Scale Calibration)
2.Target-less Dynamic Calibration (Rectification Calibration)
Refer to the user guide of Intel® RealSense™ Dynamic Calibrator for further information.

Statistical image analysis
https://math.dartmouth.edu/~m70s20/ImageAnalysis.pdf
Statistical Properties of Images
http://www.iste.co.uk/data/doc_noyoaiyjtdiw.pdf
Introduction to statistical methods in signal and imageprocessing
https://hal.archives-ouvertes.fr/cel-01423624/file/IntroBayesFF.pdf

Need to
1. scale one+ image to 720, 1280, 3. Depth map (to be obtained) to 720, 1280, before presenting to network.

Got files:

$ tar tvf pointcloud.tar.gz 
-rw-r--r-- daniel/daniel  2607 2021-04-26 21:54 read_zdf.py
-rw-rw-r-- daniel/daniel 9216128 2021-04-26 21:54 copy_data_rgba.npy
-rw-rw-r-- daniel/daniel 27648128 2021-04-26 21:54 copy_data_xyz.npy
-rw-r--r-- daniel/daniel  2764928 2021-04-26 21:59 color_img.npy
-rw-r--r-- daniel/daniel  7372928 2021-04-26 21:59 input_depth_orig.npy

****************
** 2021.04.27 **
****************

Preprocessing for Zivid One+ files.

D415 image data types and dimensions.

            size            datatype
D415
input_image (720, 1280, 3)  uint8
input_depth (720, 1280)     float64
ZividOne+
input_image (1200, 1920, 4) uint8
input_depth (1200, 1920, 3) float32


Pre-processing of Zivid One+ files:

1. Resize to match D415

from skimage.transform import resize
rgb_resized = resize(rgb, (720, 1280))
# This might take care of nans

2. Remove last dimension of rgb file to match 720 * 1280 * 3

Figure out what to do with input_depth, this needs to be flattened somehow - (1200, 1920, 3) to (1200, 1920)

Looking at this on D415xZividOnePlus.ipynb colab, more clues might exist in ClearGrasp codebase.

****************
** 2021.04.29 **
****************

We are at the stage where we will probably be able to use the Z1+ with the ClearGrasp pipeline.
The next question is, can the ClearGrasp output then be used in the Z1+ pipeline? This would require correct array dimensions - probably (1200, 1920, 3). Given the output of the convolutional autoencoders is something in the region of 300 * 200 (TBC), there is some work to be done here in the way of resizing and interpolation.
We might not get that far, so might have to leave the second part as future work, which may include redesigning autoencoders to adapt output geometries, as time is running out. We need this results wrapped up by the last week of May, camera shipped back in June plus results and discussion write-up to return at the same time camera returns, with full dissertation wrapped up by the end of June.

Useful discussion, specially converting between dimensions:
https://stackoverflow.com/questions/51350493/convertion-of-ply-format-to-pcd-format

****************
** 2021.05.01 **
****************

Tasks for today:

1. Visualise realsense depth output.
It turns out we have done this, between local jupyter notebook PointCloudExploratoryDataAnalysis and colab notebook D415xZividOnePlus.

The one question that remained was how to turn z1+ depth from three dimensional to 2 dimensional array. The naive way is to discard two of the last 3 dimensions, then squeeze the array.

Looking at:
https://github.com/zivid/zivid-python/blob/master/modules/zivid/point_cloud.py

def copy_data(self, data_format):

there is a 2d option ("z") for depth:

        Supported data formats:
        xyz:        ndarray(Height,Width,3) of float
        xyzw:       ndarray(Height,Width,4) of float
        z:          ndarray(Height,Width)   of float
        rgba:       ndarray(Height,Width,4) of uint8
        snr:        ndarray(Height,Width)   of float
        xyzrgba:    ndarray(Height,Width)   of composite dtype (accessed with e.g. arr["x"])

Further in:

https://raw.githubusercontent.com/zivid/zivid-python-samples/master/source/applications/advanced/create_depth_map.py

def _point_cloud_to_cv_z(point_cloud):

The function call is shown:

depth_map = point_cloud.copy_data("z")

while in the main() section we see:

point_cloud = frame.point_cloud()

So at this point we should be able to say:
input_depth = point_cloud.copy_data("z").

At this point we need to resize the z1+ input_depth from (1200, 1920) to (720,1280).

We should also be able to get rgba from the same frame:

rgba = point_cloud.copy_data("rgba")

and discard the "a" alpha/transparency channel, then resize.

****************
** 2021.05.02 **
****************

The capture is partially working, except for input_depth_mapped, using the Z1+, with camera intrinsics borrowed from the D415. Next we are extracting the camera extrinsics using c++ libraries, as described here:

https://github.com/zivid/zivid-cpp-samples

This should be revisited at some point to do the full install with Eigen 3, PCL and OpenCV, to leverage anything that has not got a Python wrapper.

Eigen was cloned and we should be able to point to the repository.
OpenCV install:
https://docs.opencv.org/master/d7/d9f/tutorial_linux_install.html
PCL install TBA

We managed to get the camera intrinsics:

~/git/zivid-cpp-samples/build(master)$ ./GetCameraIntrinsics 
Connecting to camera
Getting camera intrinsics
CameraIntrinsics:
  CameraMatrix:
    CX: 963.131469726562
    CY: 595.361694335938
    FX: 2763.10400390625
    FY: 2763.77685546875

And modified the code in:
~/git/cleargrasp/live_demo/live_zivid_demo.py

but it turns out they are not used.

So returning to the issue of input_depth_mapped not displaying the masks (masks are displayed when using the D415), we need to compare the outputs. We have the set of files in rgb_depth_z1_d15.tar.gz to be 
https://colab.research.google.com/drive/1v9oTdrVxKyg_PICimRSGIVidyAIzajQ1#scrollTo=5chG3De84BLa

TBC 2021.05.04

****************
** 2021.05.04 **
****************

Incorrect D415 files uploaded: d415_color_img.npy and d415_input_depth.npy, dimension is (2,) in both cases double check.

****************
** 2021.05.05 **
****************

Analysing and comparing D415 and Z1+ depth files, still using the colab notebook. D415 seems to have more half tones.

****************
** 2021.05.06 **
****************

We need to reset this now we know how to get the correct size array with the Z1+ library.
We want both the input depth and output depth (720, 1280) numpy arrays, for both cameras.
So we end up with 6 files to analyse, two RBG files, two depth files pre-processed, to depth files post-processed.

****************
** 2021.05.07 **
****************

Continuing from previous day, plus write up in methods. Used the Intel RealSense Viewer to view 2D and 3D images from D415 camera.

****************
** 2021.05.08 **
****************

Today we take a step back to consider the cleargrasp model and qualitative analysis. Having used the RealSense Viewer, we can see 3D renderings such as point clouds plus images.
The cleargrasp paper provides 2D images showing input and (predicted network) output depths, while the script live_demo.py provides a means of extracting the predicted depth. By combining image and depth
RBG (H, W, 3)
D (H, W) ~ (H, W, 1)
RGB + D = (H, W, 4)
We have the 4D array required to render RGBD in 3D space and by doing so, enabling qualitative analysis. Quantitative analysis not being possible because, with the streaming camera image, there is no ground truth geometry.
Two 3D renderings (using tools such as MeshLab), of the original RGBD data, and of the processed data, with incorporated output depth, should provide a means for qualitative analysis of the cleargrasp model, which is provided in 2D in the article, but not in 3D, which is fair enough, given the paper is in 2D format.
The same can then be done for the Z1+ camera, and this should then uncover any additional processing required in terms of aligning camera outputs (Z1+ w.r.t. D415), such that the model is usable for the Z1+. 
Ideally we would somehow be able to integrate this into Zivid Studio or RealSense Viewer. Given that both are probably using the same open source computer vision software, we might have time to do it.

We can display a point cloud with OpenCV in:
~/git/cleargrasp/livedemo/viz/display-point-cloud.py

Next we need to build our own point cloud with input and network predicted input, then build the point cloud see:
https://stackoverflow.com/questions/59590200/generate-point-cloud-from-depth-image

****************
** 2021.05.16 **
****************

Got error when running ~/git/cleargrasp/live_demo/realsense/build/realsense:

no realsenselib (something like that) found.

Fixed by recompiling realsense

cd build
cmake ..
make

Continuining from 2021.05.08, if we export a point cloud from the realsense-viewer, we can display it (e.g. output_meshing_normals.ply) in 3D ok using our display-point-cloud.py script. The next step is to create, in turn using the open3d library. Next we want to generate the same point cloud from the colour and depth camera output, preferably using some python library, for easy duplication.

D415 depth is not rendering as expected. Need to look at data, maybe normalise as suggested in:

https://www.google.com/search?channel=fs&client=ubuntu&q=normalize+depth+image

https://github.com/BerkeleyAutomation/sd-maskrcnn/issues/6

https://github.com/BerkeleyAutomation/perception/blob/c5c07f8e26964fd006db59d5b2d2385420eb7789/perception/image.py#L1478

****************
** 2021.05.22 **
****************

Ran tests, documented in results appendix, on running RealSense Viewer, Zivid Studio, outputting point clouds, and displaying with Open3D.

****************
** 2021.05.23 **
****************

Ran tests 4 to 7 in results appendix.

****************
** 2021.05.30 **
****************

We reach a stage were a better understanding of point clouds and depth is required.

To perform qualitative analysis on the depth completing, we need to visualise the point cloud before and after the prediction. This involves creating a point cloud from RGB and depth, with can be acquired with both cameras. Once RGB and depth have been generated, there are points to consider. Matching the point cloud to RGB is likely to involve upsampling or downsampling, especially with the predicted depth, which is smaller than the input depth. Also there is the question of the Projection Matrix, "used to project vertices of 3D objects onto the screen in order to create images of these objects that follow the rules of perspective."
https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix

****************
** 2021.06.20 **
****************

We briefly looked at camera calibration at it seems the D415 is factory calibrated, and will not need recalibration unless is suffers a hard knock such as falling on the ground:
https://github.com/IntelRealSense/librealsense/issues/2329

In the Zivid One+ knowledge base, only calibration mentioned is hard-eye calibration:
https://www.zivid.com/quick-start-guide-for-zivid-one-plus
Which does not apply to this study, since there is no robotic arm or end effector.

Got a rendering closer to zivid studio by using the camera intrinsics and using this code:
https://stackoverflow.com/questions/60648588/can-open3d-visualize-a-point-cloud-in-rgb-mode

We now have the script to do qualitative analysis, with the point clouds rendered with RGB image and camera intrisics, so in line with D415 RealSense Viewer and Zivid Studio renderings.
Next we'll create command line parameters for camera, rgb and depth python array files, create the renderings for input depth and predicted depth, create front, side, top and perspective views of both cases, as well as RealSense Viewer and Zivid Studio. This should give us four sets of four images, to then be added to appendix and discussion.

We have all the screen captures in Pictures and the numpy arrays in viz/data so should be able to produce all views in the next session.
Todo:
1. Finish cropping d415 images
2. Assembled isometric views
3. Build point clouds
4. Generate isometric views

****************
** 2021.06.27 **
****************

Added RealSense Viewer and Zivid Studio views, showing glass surfaces captures incorrectly.

TODOS for tomorrow:
1. Add views for scaled input:
python create-point-cloud.py --rgb=./data/zivid/zivid_color_img.npy --depth=./data/zivid/zivid_input_depth_scaled.npy --camera=zivid --depth_type=acquired
2. Rerun with Zivid One+ and scaled values (corrected zivid_utils.py
3. Complete create-point-cloud.py (scale image when rendering predicted depth).
















































